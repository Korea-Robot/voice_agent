"""
This file contains the implementation of a VoiceAgent class that interacts with the OpenAI API to perform speech-to-text, text generation, and text-to-speech tasks. It also includes an example usage of the VoiceAgent class.

The VoiceAgent class has the following methods:
- speech_to_text: Converts an audio file to text using the OpenAI API.
- text_generation: Generates a response to a user's question using the OpenAI API.
- text_to_speech: Converts text to speech using the OpenAI API.
- save_audio: Saves the generated audio content to a file.

Example usage:
- Create an instance of the VoiceAgent class with an API key.
- Use the speech_to_text method to convert an audio file to text.
- Use the text_generation method to generate a response to the user's question.
- Use the text_to_speech method to convert the generated response to speech.
- Use the  save_audio method to save the generated audio content to a file.
"""

from openai import OpenAI
import pyaudio
import rclpy.node
import webrtcvad
from pydub import AudioSegment
import numpy as np
from scipy import signal

import subprocess
from typing import Union
from threading import Lock, Thread
from array import array
from struct import pack
import wave
import time
import collections
import sys
import os

import abc
import argparse

import rclpy

DEFAULT_PERSONA = (
    "You are Vision 60, a quadruped robot designed to navigate and adapt in complex environments. "
    "You understand Korean, English. You can answer questions freely like a helpful assistant."
    "you will response to my question up to 50 tokens."
)

COMMAND_PERSONA = (
    "You are Vision 60, a quadruped robot designed to navigate and adapt in complex environments. "
    "You understand both basic **state change** commands and **motion commands**.\n\n"

    "1. **State Change Commands**:\n"
    "These are simple behavioral commands. Respond with only the number:\n"
    "- 'ÏïâÏïÑ' (sit) ‚Üí 0\n"
    "- 'ÏùºÏñ¥ÎÇò' (stand) ‚Üí 1\n"
    "- 'Í±∑Í∏∞ Î™®Îìú' (walk) ‚Üí 2\n\n"

    "### Examples:\n"
    "'ÏïâÏïÑÏ§ò' ‚Üí 0\n"
    "'ÏùºÏñ¥ÎÇòÏûê' ‚Üí 1\n"
    "'Í±∑Í∏∞ ÏãúÏûëÌï¥' ‚Üí 2\n\n"

    "2. **Motion Commands (Twist Publishing)**:\n"
    "Respond with a list of 4 values:\n"
    "[vx, vy, vyaw, duration]  (all in float)\n\n"

    "Speed Interpretation:\n"
    "- Default speed: 0.4\n"
    "- If command contains 'Îπ†Î•¥Í≤å' or 'ÏµúÎåÄÌïú Îπ†Î•¥Í≤å': speed = 0.7\n"
    "- If command contains 'ÎäêÎ¶¨Í≤å' or 'Ï≤úÏ≤úÌûà': speed = 0.2\n\n"

    "Direction Interpretation:\n"
    "- vx > 0 ‚Üí forward\n"
    "- vx < 0 ‚Üí backward\n"
    "- vy > 0 ‚Üí move left\n"
    "- vy < 0 ‚Üí move right\n"
    "- vyaw > 0 ‚Üí rotate left\n"
    "- vyaw < 0 ‚Üí rotate right\n\n"
    "- duration: how long to move (seconds)\n\n"

    "Rules:\n"
    "- Respond ONLY with the list.\n"
    "- No explanation or extra text.\n"
    "- You understand Korean, English.\n"
    
    "### Examples:\n"
    "'ÏïûÏúºÎ°ú Í∞Ä' ‚Üí [0.4, 0.0, 0.0, 1]\n"
    "'ÏïûÏúºÎ°ú 3Ï¥à ÎèôÏïà Í∞Ä' ‚Üí [0.4, 0.0, 0.0, 3]\n"
    "'Îπ†Î•¥Í≤å ÏïûÏúºÎ°ú Í∞Ä' ‚Üí [0.7, 0.0, 0.0, 1]\n"
    "'ÎäêÎ¶¨Í≤å ÏïûÏúºÎ°ú 2Ï¥à ÎèôÏïà Í∞Ä' ‚Üí [0.2, 0.0, 0.0, 2]\n"
    "'ÏôºÏ™ΩÏúºÎ°ú Í∞Ä' ‚Üí [0.0, 0.4, 0.0, 1]\n"
    "'Ïò§Î•∏Ï™ΩÏúºÎ°ú Í∞Ä' ‚Üí [0.0, -0.4, 0.0, 1]\n"
    "'Ï≤úÏ≤úÌûà Ïò§Î•∏Ï™ΩÏúºÎ°ú 5Ï¥à ÎèôÏïà Í∞Ä' ‚Üí [0.0, -0.4, 0.0, 5]\n"
    "'Ïò§Î•∏Ï™ΩÏúºÎ°ú ÌöåÏ†ÑÌï¥' ÎòêÎäî 'Ïò§Î•∏Ï™ΩÏúºÎ°ú ÎèåÏïÑ' ‚Üí [0.0, 0.0, -0.4, 1]\n"
    "'Ïò§Î•∏Ï™ΩÏúºÎ°ú 5Ï¥à ÎèôÏïà ÌöåÏ†ÑÌï¥' ÎòêÎäî 'Ïò§Î•∏Ï™ΩÏúºÎ°ú 5Ï¥à ÎèôÏïà ÎèåÏïÑ' ‚Üí [0.0, 0.0, -0.4, 5]\n"
    "'ÏôºÏ™ΩÏúºÎ°ú ÌöåÏ†ÑÌï¥' ÎòêÎäî 'ÏôºÏ™ΩÏúºÎ°ú ÎèåÏïÑ' ‚Üí [0.0, 0.0, 0.4, 1]\n"
    "'ÏôºÏ™ΩÏúºÎ°ú 5Ï¥à ÎèôÏïà ÌöåÏ†ÑÌï¥' ÎòêÎäî 'ÏôºÏ™ΩÏúºÎ°ú 5Ï¥à ÎèôÏïà ÎèåÏïÑ' ‚Üí [0.0, 0.0, 0.4, 5]\n"
    "'Îπ†Î•¥Í≤å ÏôºÏ™ΩÏúºÎ°ú 3Ï¥à ÎèôÏïà ÌöåÏ†ÑÌï¥' ‚Üí [0.0, 0.0, 0.7, 3]\n"
    "'Îπ†Î•¥Í≤å Ïò§Î•∏Ï™ΩÏúºÎ°ú 3Ï¥à ÎèôÏïà ÌöåÏ†ÑÌï¥' ‚Üí [0.0, 0.0, -0.7, 3]\n"
    "'ÎäêÎ¶¨Í≤å Ïò§Î•∏Ï™ΩÏúºÎ°ú ÌöåÏ†ÑÌï¥' ‚Üí [0.0, 0.0, -0.2, 1]\n"
    "'Î©àÏ∂∞' ÎòêÎäî 'Ï†ïÏßÄ' ‚Üí [0.0, 0.0, 0.0, 1]\n"
    "'Îí§Î°ú Í∞Ä' ‚Üí [-0.4, 0.0, 0.0, 1]\n"
    "'Ï≤úÏ≤úÌûà Îí§Î°ú Í∞Ä' ‚Üí [-0.2, 0.0, 0.0, 1]\n"
    "'Ï≤úÏ≤úÌûà Îí§Î°ú 4Ï¥à ÎèôÏïà Í∞Ä' ‚Üí [-0.2, 0.0, 0.0, 4]\n"
    "'Îπ†Î•¥Í≤å ÏôºÏ™ΩÏúºÎ°ú 2Ï¥à ÎèôÏïà Í∞Ä' ‚Üí [0.0, 0.7, 0.0, 2]\n"

    "- Return int -1 if the command is invalid or ambiguous.\n\n"
)

# Audio settings
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 48000
CHUNK_DURATION_MS = 20  # supports 10, 20 and 30 (ms)
PADDING_DURATION_MS = 1000  # 1 sec jugement
CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)  # chunk to read
NUM_PADDING_CHUNKS = int(PADDING_DURATION_MS / CHUNK_DURATION_MS)
NUM_WINDOW_CHUNKS = int(300 / CHUNK_DURATION_MS)  # 300 ms/ 30ms  ge
NUM_WINDOW_CHUNKS_END = NUM_WINDOW_CHUNKS # originally, it was NUM_WINDOW_CHUNKS * 2


class VoiceAgent:

    def __init__(
        self,
        api_key=None,
        base_path=os.path.dirname(os.path.abspath(__file__)),
        mic_index=0,
    ):
        """
        Initializes a VoiceAgent object with the provided API key.

        Args:
            api_key (str): The API key for accessing the OpenAI API.
            base_path (str, optional): The base path for file operations. Defaults to the directory of the current file.
        """

        if api_key is None:
            self.client = OpenAI()  # Get from environment variable
        else:
            self.client = OpenAI(api_key=api_key)

        # VAD ÎØºÍ∞êÎèÑ: 0(Í∞ÄÏû• ÎÇÆÏùå) ~ 3(Í∞ÄÏû• ÎÜíÏùå)
        # Ïù∏ÏãùÏù¥ Ïûò ÏïàÎê† ÎïåÎäî 3ÏúºÎ°ú ÎÜíÏù¥Í∏∞
        self.vad = webrtcvad.Vad(3)  # ÏµúÍ≥† ÎØºÍ∞êÎèÑÎ°ú ÏÑ§Ï†ï (Ïù∏Ïãù Í∞úÏÑ†)
        self.base_path = base_path
        self.pa = pyaudio.PyAudio()

        # self.speaker_devices = ["plughw:0,0", "plughw:1,0"]
        self.speaker_devices = self.get_speaker_devices()
        self.microphone_devices = self.get_microphone_devices()

        print("--------------------------------")
        print(f"Speaker Devices: {self.speaker_devices}")
        print(f"Microphone Devices: {self.microphone_devices}")
        print("--------------------------------")

        self.sound_lock = Lock()
        self.command_mode = False
        self.active_twist_process = None  # ÌòÑÏû¨ Twist ÌçºÎ∏îÎ¶¨Ïãú ÌîÑÎ°úÏÑ∏Ïä§ Ï†ÄÏû•

        self.ros_node = rclpy.node

        self.ensure_mode = self.ros_node.Service 

        try:
            # test.pyÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ïä§Ìä∏Î¶º Ïó¥Í∏∞
            device_index = self.microphone_devices
            actual_rate = RATE  # Í∏∞Î≥∏Í∞í
            
            # Ïû•Ïπò Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞ (test.py Î∞©Ïãù)
            if device_index is not None:
                dev_info = self.pa.get_device_info_by_host_api_device_index(0, device_index)
                actual_rate = int(dev_info.get('defaultSampleRate', RATE))
                if actual_rate != RATE:
                    print(f"[INFO] Ïû•Ïπò ÏÉòÌîå Î†àÏù¥Ìä∏ ({actual_rate}Hz)Ïóê ÎßûÏ∂∞ Ï°∞Ï†ïÌï©ÎãàÎã§.")
                print(f"[INFO] ÏÑ†ÌÉù Ïû•Ïπò: {dev_info.get('name', 'Unknown')}")
                print(f"[INFO] ÏÉòÌîåÎßÅ Î†àÏù¥Ìä∏: {actual_rate}Hz")
            else:
                print("‚ö†Ô∏è  ÎßàÏù¥ÌÅ¨ Ïû•ÏπòÍ∞Ä NoneÏûÖÎãàÎã§. Í∏∞Î≥∏ Ïû•ÏπòÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
            
            # CHUNK_SIZEÎ•º Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏Ïóê ÎßûÍ≤å Ï°∞Ï†ï
            actual_chunk_size = int(actual_rate * CHUNK_DURATION_MS / 1000)
            
            # Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏ÏôÄ Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞Î•º Ïù∏Ïä§ÌÑ¥Ïä§ Î≥ÄÏàòÎ°ú Ï†ÄÏû•
            self.actual_rate = actual_rate
            self.actual_chunk_size = actual_chunk_size
            
            # VADÎ•º ÏúÑÌïú ÏÉòÌîå Î†àÏù¥Ìä∏ Í≤∞Ï†ï (webrtcvadÎäî 8000, 16000, 32000, 48000Îßå ÏßÄÏõê)
            # Í∞ÄÏû• Í∞ÄÍπåÏö¥ ÏßÄÏõê Î†àÏù¥Ìä∏Î°ú ÏÑ†ÌÉùÌïòÍ±∞ÎÇò 48000ÏúºÎ°ú Î¶¨ÏÉòÌîåÎßÅ
            vad_supported_rates = [8000, 16000, 32000, 48000]
            if actual_rate in vad_supported_rates:
                self.vad_rate = actual_rate
                self.needs_resample = False
            else:
                # 48000HzÎ°ú Î¶¨ÏÉòÌîåÎßÅ (Í∞ÄÏû• ÏùºÎ∞òÏ†ÅÏù∏ Í≥†ÌíàÏßà Î†àÏù¥Ìä∏)
                self.vad_rate = 48000
                self.needs_resample = True
                print(f"[INFO] VADÎ•º ÏúÑÌï¥ Ïò§ÎîîÏò§Î•º {actual_rate}Hz ‚Üí {self.vad_rate}HzÎ°ú Î¶¨ÏÉòÌîåÎßÅÌï©ÎãàÎã§.")
            
            self.stream = self.pa.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=actual_rate,  # Ïû•ÏπòÏùò Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏ ÏÇ¨Ïö©
                input=True,
                input_device_index=device_index,  # NoneÏù¥Î©¥ Í∏∞Î≥∏ Ïû•Ïπò ÏÇ¨Ïö©
                start=False,
                frames_per_buffer=actual_chunk_size,  # Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏Ïóê ÎßûÏ∂ò Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞
            )
        except Exception as e:
            raise RuntimeError(f"üé§ ÎßàÏù¥ÌÅ¨ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}\nÌåÅ: 'pulse'ÎÇò 'default' Ïû•ÏπòÎ•º ÏÇ¨Ïö©Ìï¥Î≥¥ÏÑ∏Ïöî.")

        os.makedirs(os.path.dirname("./media/audio.wav"), exist_ok=True)

    def get_speaker_devices(self):
        devices = []
        for i in range(self.pa.get_device_count()):
            device_info = self.pa.get_device_info_by_index(i)
            name = device_info["name"]
            max_output = device_info["maxOutputChannels"]

            # Ï∂úÎ†• Í∞ÄÎä•Ìïú Ïû•Ïπò + ÌïÑÌÑ∞ÎßÅÎêú Ïù¥Î¶Ñ Ï°∞Í±¥Îßå Îì±Î°ù
            if max_output > 0 and (name == "default" or "plughw:" in name):
                devices.append(name)

        return devices

    def get_microphone_devices(self):
        """
        ÎßàÏù¥ÌÅ¨ Ïû•ÏπòÎ•º Ï∞æÏïÑ Ïù∏Îç±Ïä§Î•º Î∞òÌôòÌï©ÎãàÎã§.
        test.py Í∏∞Î∞òÏùò Í∞ÑÎã®Ìïú Ï†ëÍ∑º Î∞©Ïãù ÏÇ¨Ïö©.
        
        Ïö∞ÏÑ†ÏàúÏúÑ:
        1. 'pulse' Ïû•Ïπò (PulseAudioÎ•º ÌÜµÌï¥ C-type ÎßàÏù¥ÌÅ¨ Ï†ëÍ∑º Í∞ÄÎä•)
        2. 'default' Ïû•Ïπò
        3. Ï≤´ Î≤àÏß∏ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏûÖÎ†• Ïû•Ïπò
        
        Returns:
            int or None: ÎßàÏù¥ÌÅ¨ Ïû•Ïπò Ïù∏Îç±Ïä§
        """
        print("=" * 60)
        print(" [Ïó∞Í≤∞Îêú Ïò§ÎîîÏò§ Ïû•Ïπò Î™©Î°ù] ")
        print("=" * 60)
        
        # test.pyÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ïû•Ïπò Î™©Î°ù Ï°∞Ìöå
        info = self.pa.get_host_api_info_by_index(0)
        numdevices = info.get('deviceCount')
        
        input_devices = []
        pulse_idx = None
        default_idx = None
        
        # test.pyÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ïû•Ïπò Î™©Î°ù ÏÉùÏÑ±
        for i in range(0, numdevices):
            device_info = self.pa.get_device_info_by_host_api_device_index(0, i)
            if device_info.get('maxInputChannels', 0) > 0:
                device_name = device_info.get('name', '')
                sample_rate = int(device_info.get('defaultSampleRate', 0))
                print(f"Index {i}: {device_name} (Default Rate: {sample_rate}Hz)")
                input_devices.append(i)
                
                # 'pulse' Ïû•Ïπò Ï∞æÍ∏∞
                if device_name.lower() == 'pulse':
                    pulse_idx = i
                
                # 'default' Ïû•Ïπò Ï∞æÍ∏∞
                if device_name.lower() == 'default':
                    default_idx = i
        
        print("=" * 60)
        
        # Ïö∞ÏÑ†ÏàúÏúÑ 1: 'pulse' Ïû•Ïπò (PulseAudioÎ•º ÌÜµÌï¥ C-type ÎßàÏù¥ÌÅ¨ Ï†ëÍ∑º)
        if pulse_idx is not None:
            device_info = self.pa.get_device_info_by_host_api_device_index(0, pulse_idx)
            print(f"‚úÖ ÏÑ†ÌÉùÎêú ÎßàÏù¥ÌÅ¨: Index {pulse_idx} - {device_info['name']}")
            print("   üí° PulseAudioÎ•º ÌÜµÌï¥ C-type ÎßàÏù¥ÌÅ¨Ïóê Ï†ëÍ∑ºÌï©ÎãàÎã§.")
            return pulse_idx
        
        # Ïö∞ÏÑ†ÏàúÏúÑ 2: 'default' Ïû•Ïπò
        if default_idx is not None:
            device_info = self.pa.get_device_info_by_host_api_device_index(0, default_idx)
            print(f"‚úÖ ÏÑ†ÌÉùÎêú ÎßàÏù¥ÌÅ¨: Index {default_idx} - {device_info['name']}")
            return default_idx
        
        # Ïö∞ÏÑ†ÏàúÏúÑ 3: Ï≤´ Î≤àÏß∏ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏûÖÎ†• Ïû•Ïπò
        if input_devices:
            device_idx = input_devices[0]
            device_info = self.pa.get_device_info_by_host_api_device_index(0, device_idx)
            print(f"‚úÖ ÏÑ†ÌÉùÎêú ÎßàÏù¥ÌÅ¨: Index {device_idx} - {device_info['name']}")
            return device_idx
        
        # ÏûÖÎ†• Ïû•ÏπòÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞
        print("‚ùå ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎßàÏù¥ÌÅ¨ Ïû•ÏπòÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        return None

    def loop(self):
        """
        Starts the main loop of the VoiceAgent.

        The loop continuously listens for audio input, converts it to text, generates a response, converts the response to speech,
        saves the speech as an audio file, and plays the audio file.

        Returns:
            None
        """
        while True:
            if self.check_break_signal():
                break

            self.listen()
            question = self.speech_to_text()
            print(f"Question: {question}")

            # Î™ÖÎ†π Î™®Îìú ÏßÑÏûÖ ÎòêÎäî Ï¢ÖÎ£å ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
            if "Î™ÖÎ†π Î™®Îìú" in question:
                self.command_mode = True
                print(f"Command Mode Activated - Command_flag: {self.command_mode}")
                continue
            elif "Í∏∞Î≥∏ Î™®Îìú" in question:
                self.command_mode = False
                print(f"Default Mode Activated - Command_flag: {self.command_mode}")

            # ÏùëÎãµ ÏÉùÏÑ±
            answer = self.text_generation(question)
            print(f"Answer: {answer}")

            # (ÏÑ†ÌÉù) Î™ÖÎ†π Î™®ÎìúÏùº Í≤ΩÏö∞ Ïà´ÏûêÎßå Ï∂îÏ∂ú
            if self.command_mode:
                try:
                    action_code = int(answer)
                    print(f"Action Code: {action_code} ‚Üí ROS2Î°ú Ï†ÑÏÜ° Ï§ë...")
                    # self.publish_ros2_command(action_code)
                except ValueError:
                    print("Î™ÖÎ†π ÌååÏã± Ïã§Ìå®: Ïà´Ïûê ÏùëÎãµ ÏïÑÎãò")

            else : 
                audio_content = self.text_to_speech(answer)
                self.save_audio(audio_content)
                self.speak()

    @abc.abstractmethod
    def check_break_signal(self):
        """
        Checks for a break signal from the user.

        Returns:
            bool: True if a break signal is received, False otherwise.
        """
        # TODO: Implement a way to check for a break signal
        # Add your code here to check for a break signal
        # For example, you can use the keyboard module to check for keyboard input
        # If a break signal is received, return True
        # Otherwise, return False
        return False
    def listen(self, output_file="./media/audio.wav"):

        """
        Listens for audio input and saves it as a WAV file.

        Args:
            output_file (str, optional): The path to save the audio file. Defaults to "./media/audio.wav".

        Returns:
            None
        """
        got_a_sentence = False
        leave = False

        ring_buffer = collections.deque(maxlen=NUM_PADDING_CHUNKS)
        triggered = False
        ring_buffer_flags = [0] * NUM_WINDOW_CHUNKS
        ring_buffer_index = 0

        ring_buffer_flags_end = [0] * NUM_WINDOW_CHUNKS_END
        ring_buffer_index_end = 0
        # WangS
        raw_data = array("h")
        index = 0
        start_point = 0
        StartTime = time.time()
        print("* recording: ")
        self.stream.start_stream()

        # ÏùåÏÑ± Ïù∏Ïãù ÎØºÍ∞êÎèÑ ÌååÎùºÎØ∏ÌÑ∞ (Ï°∞Ï†ï Í∞ÄÎä•)
        # Ïù∏ÏãùÏù¥ Ïûò ÏïàÎê† Îïå: amplitude_threshold ÎÇÆÏ∂îÍ∏∞, min_consecutive_voiced ÎÇÆÏ∂îÍ∏∞
        amplitude_threshold = 500  # ÏùåÎüâ ÏûÑÍ≥ÑÍ∞í (ÎÇÆÏùÑÏàòÎ°ù ÏûëÏùÄ Î™©ÏÜåÎ¶¨ÎèÑ Ïù∏Ïãù, Í∏∞Î≥∏: 1000)
        min_consecutive_voiced = 3  # ÏµúÏÜå Ïó∞ÏÜç Î∞úÌôî ÌîÑÎ†àÏûÑ (ÎÇÆÏùÑÏàòÎ°ù Îπ†Î•∏ ÏãúÏûë, Í∏∞Î≥∏: 5)
        consecutive_voiced = 0

        # Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏ÏôÄ Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ ÏÇ¨Ïö©
        actual_rate = getattr(self, 'actual_rate', RATE)
        actual_chunk_size = getattr(self, 'actual_chunk_size', CHUNK_SIZE)
        vad_rate = getattr(self, 'vad_rate', 48000)
        needs_resample = getattr(self, 'needs_resample', False)
        
        while not got_a_sentence and not leave:
            chunk = self.stream.read(actual_chunk_size, exception_on_overflow=False)
            # add WangS
            samples = array("h", chunk)
            raw_data.extend(samples)
            index += actual_chunk_size
            # VAD + ÏùåÎüâ Í∏∞Î∞ò Ïù¥Ï§ë Ï°∞Í±¥
            # ÌèâÍ∑† Ï†àÎåÄÍ∞í(RMS Ïú†ÏÇ¨) Í≥ÑÏÇ∞ÏúºÎ°ú ÏûëÏùÄ Ïû°Ïùå ÏñµÏ†ú
            if len(samples) > 0:
                avg_abs = sum(abs(s) for s in samples) / len(samples)
            else:
                avg_abs = 0
            
            # VADÎ•º ÏúÑÌïú Ïò§ÎîîÏò§ Ï≤òÎ¶¨
            if needs_resample:
                # Î¶¨ÏÉòÌîåÎßÅÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞
                samples_np = np.array(samples, dtype=np.int16)
                num_samples = len(samples_np)
                # scipy.signal.resample ÏÇ¨Ïö©
                resampled = signal.resample(samples_np, int(num_samples * vad_rate / actual_rate))
                resampled = resampled.astype(np.int16)
                vad_chunk = resampled.tobytes()
            else:
                # Î¶¨ÏÉòÌîåÎßÅÏù¥ ÌïÑÏöî ÏóÜÎäî Í≤ΩÏö∞
                vad_chunk = chunk
            
            vad_active = self.vad.is_speech(vad_chunk, vad_rate)  # VAD ÏßÄÏõê Î†àÏù¥Ìä∏ ÏÇ¨Ïö©
            active = vad_active and (avg_abs >= amplitude_threshold)

            # Ïó∞ÏÜç Î∞úÌôî ÌîÑÎ†àÏûÑ Ïπ¥Ïö¥Ìä∏
            if active:
                consecutive_voiced += 1
            else:
                consecutive_voiced = 0

            ring_buffer_flags[ring_buffer_index] = 1 if active else 0
            ring_buffer_index += 1

            ring_buffer_index %= NUM_WINDOW_CHUNKS

            ring_buffer_flags_end[ring_buffer_index_end] = 1 if active else 0
            ring_buffer_index_end += 1
            ring_buffer_index_end %= NUM_WINDOW_CHUNKS_END

            # start point detection
            if not triggered:
                ring_buffer.append(chunk)
                num_voiced = sum(ring_buffer_flags)
                # ÏãúÏûë Ìä∏Î¶¨Í±∞ ÏûÑÍ≥ÑÍ∞í (ÎÇÆÏùÑÏàòÎ°ù ÏâΩÍ≤å ÏãúÏûë, Í∏∞Î≥∏: 0.9)
                start_trigger_threshold = 0.7  # 70%Î°ú ÎÇÆÏ∂§ (Ïù∏Ïãù Í∞úÏÑ†)
                if num_voiced > start_trigger_threshold * NUM_WINDOW_CHUNKS and consecutive_voiced >= min_consecutive_voiced:
                    sys.stdout.write(" Open ")
                    StartTime = time.time()
                    triggered = True
                    start_point = index - actual_chunk_size * 20  # start point
                    ring_buffer.clear()

            # end point detection
            else:
                ring_buffer.append(chunk)
                num_unvoiced = NUM_WINDOW_CHUNKS_END - sum(ring_buffer_flags_end)
                # Ï¢ÖÎ£å Ìä∏Î¶¨Í±∞ ÏûÑÍ≥ÑÍ∞í (ÎÜíÏùÑÏàòÎ°ù Îçî Ïò§Îûò ÎÖπÏùå, Í∏∞Î≥∏: 0.90)
                end_trigger_threshold = 0.95  # 95%Î°ú ÎÜíÏûÑ (Îçî Ïò§Îûò ÎÖπÏùå)
                max_recording_time = 10  # ÏµúÎåÄ ÎÖπÏùå ÏãúÍ∞Ñ (Ï¥à)
                if (
                    num_unvoiced > end_trigger_threshold * NUM_WINDOW_CHUNKS_END
                    or (time.time() - StartTime) > max_recording_time
                ):
                    sys.stdout.write(" Close \n")
                    triggered = False
                    got_a_sentence = True

            sys.stdout.flush()

        self.stream.stop_stream()
        print("* done recording")
        got_a_sentence = False

        # write to file
        raw_data.reverse()
        for index in range(start_point):
            raw_data.pop()
        raw_data.reverse()
        raw_data = self.normalize(raw_data)
        # Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏ ÏÇ¨Ïö©
        actual_rate = getattr(self, 'actual_rate', RATE)
        self.record_to_file(output_file, raw_data, 2, sample_rate=actual_rate)

    def manual_listen(self, duration=5, output_file="./media/audio.wav"):
        """
        Listens for audio input and saves it as a WAV file.

        Args:
            duration (int, optional): The duration to listen for in seconds. Defaults to 5.
            output_file (str, optional): The path to save the audio file. Defaults to "./media/audio.wav".

        Returns:
            None
        """
        stream = self.pa.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            start=False,
            frames_per_buffer=CHUNK_SIZE,
        )

        raw_data = array("h")
        print("* recording: ")
        stream.start_stream()

        for _ in range(int(RATE / CHUNK_SIZE * duration)):
            chunk = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            raw_data.extend(array("h", chunk))

        stream.stop_stream()
        print("* done recording")

        raw_data = self.normalize(raw_data)
        self.record_to_file(output_file, raw_data, 2)

    def normalize(self, snd_data):
        "Average the volume out"
        MAXIMUM = 32767  # 16384
        times = float(MAXIMUM) / max(abs(i) for i in snd_data)
        r = array("h")
        for i in snd_data:
            r.append(int(i * times))
        return r

    def record_to_file(self, path, data, sample_width, sample_rate=None):
        """
        Records audio data to a file.

        Args:
            path (str): The path to save the audio file.
            data (list): The audio data.
            sample_width (int): The sample width of the audio data.
            sample_rate (int, optional): The sample rate. If None, uses RATE.

        Returns:
            None
        """
        if sample_rate is None:
            sample_rate = getattr(self, 'actual_rate', RATE)
        
        data = pack("<" + ("h" * len(data)), *data)
        wf = wave.open(path, "wb")
        wf.setnchannels(1)
        wf.setsampwidth(sample_width)
        wf.setframerate(sample_rate)  # Ïã§Ï†ú ÏÉòÌîå Î†àÏù¥Ìä∏ ÏÇ¨Ïö©
        wf.writeframes(data)
        wf.close()

    def speech_to_text(self, audio_file="./media/audio.wav"):
        """
        Converts an audio file to text using the OpenAI API.

        Args:
            audio_file (str, optional): The path to the audio file. Defaults to "./audio.wav".

        Returns:
            str: The transcribed text.
        """
        audio_file = open(audio_file, "rb")
        stt = self.client.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe", file=audio_file
        )
        return stt.text

    def text_generation(self, question):
        """
        Generates a response to a user's question using the OpenAI API.

        Args:
            question (str): The user's question.

        Returns:
            str: The generated response.
        """
        persona = COMMAND_PERSONA if self.command_mode else DEFAULT_PERSONA
        response = self.client.chat.completions.create(
            model="gpt-5-nano",
            max_completion_tokens=600,
            messages=[
                {"role": "system", "content": persona},
                {"role": "user", "content": question},
            ],
        )

        return response.choices[0].message.content

    def text_to_speech(self, text):
        """
        Converts text to speech using the OpenAI API.

        Args:
            text (str): The text to convert to speech.

        Returns:
            bytes: The generated audio content.
        """
        speech_output = self.client.audio.speech.create(
            model="tts-1",
            voice="onyx",
            response_format="wav",
            input=text,
        )

        return speech_output.content

    def save_audio(self, content, audio_file="./media/output.wav"):
        """
        Saves the generated audio content to a file.

        Args:
            content (bytes): The audio content to save.
            audio_file (str, optional): The path to save the audio file. Defaults to "./output.wav".

        Returns:
            bool: True if the audio file was saved successfully, False otherwise.
        """
        if content:
            with open(audio_file, "wb") as f:
                f.write(content)
            return True
        else:
            return False

    def speak(self, audio_file="./media/output.wav"):
        """
        Plays the audio file.

        Args:
            audio_file (str, optional): The path to the audio file. Defaults to "./output.wav".

        Returns:
            None
        """

        # self.increase_volume_pydub(audio_file, audio_file, 10)
        Thread(target=self.increase_volume_pydub, args=(audio_file, audio_file, 10)).start()

        with self.sound_lock:
            threads = []
            for device in self.speaker_devices:
                # Start a thread for each device
                thread = Thread(
                    target=self.play_sound_on_device, args=(device, audio_file)
                )
                thread.start()
                threads.append(thread)

            # Wait for all threads to complete
            for thread in threads:
                thread.join()

    def increase_volume_pydub(self, wave_file_path, output_file_path, dB):
        sound = AudioSegment.from_wav(wave_file_path)
        louder_sound = sound + dB  # Increase volume by dB decibels
        louder_sound.export(output_file_path, format="wav")

    def play_sound_on_device(self, device, sound_file):
        command = ["aplay", "-D", device, sound_file]
        try:
            subprocess.run(command, check=True)
            print(f"‚úÖ Ïä§ÌîºÏª§ Ïû¨ÏÉù ÏÑ±Í≥µÌïú ÎîîÎ∞îÏù¥Ïä§: {device}")
        except subprocess.CalledProcessError as e:
            print(f"Error playing sound on {device}: {e}")

    def publish_ros2_command(self, command: Union[int, dict]):
        if isinstance(command, int):
            # Í∏∞Ï°¥ ÏÉÅÌÉú Ï†ÑÌôò Î™ÖÎ†π
            cmd = [
                "ros2", "service", "call",
                "/ensure_mode",
                "ghost_manager_interfaces/srv/EnsureMode",
                f"{{field: 'action', valdes: {command}}}"
            ]
            try:
                subprocess.run(cmd, check=True)
                print(f"[ACTION] Î™ÖÎ†π Ï†ÑÏÜ° ÏôÑÎ£å: {command}")
            except subprocess.CalledProcessError as e:
                print(f"[ERROR] ÏÉÅÌÉú Ï†ÑÌôò Î™ÖÎ†π Ïã§Ìå®: {e}")

        elif isinstance(command, dict):
            try:
                # Ïù¥Ï†Ñ ÌçºÎ∏îÎ¶¨ÏÖî Ï¢ÖÎ£å Ï≤òÎ¶¨ Ï∂îÍ∞Ä!
                if self.active_twist_process:
                    print("[INFO] Ïù¥Ï†Ñ ÌçºÎ∏îÎ¶¨Ïãú ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å")
                    self.active_twist_process.terminate()
                    self.active_twist_process = None

                topic = command.get("topic")
                msg_type = command.get("msg_type")
                msg = command.get("msg", {})
                rate = str(command.get("rate", 10))  # Í∏∞Î≥∏ 10Hz

                linear = msg.get("linear", {})
                angular = msg.get("angular", {})

                # Twist Î©îÏãúÏßÄÎ•º Î¨∏ÏûêÏó¥Î°ú YAML Ìè¨Îß∑ Íµ¨ÏÑ±
                twist_msg = (
                    f"{{linear: {{x: {linear.get('x', 0.0)}, y: {linear.get('y', 0.0)}, z: {linear.get('z', 0.0)}}}, "
                    f"angular: {{x: {angular.get('x', 0.0)}, y: {angular.get('y', 0.0)}, z: {angular.get('z', 0.0)}}}}}"
                )

                cmd = [
                    "ros2", "topic", "pub", "--rate", rate,
                    topic,
                    msg_type,
                    twist_msg
                ]

                print(f"[TWIST-CLI] Ïã§Ìñâ Î™ÖÎ†π: {' '.join(cmd)}")
                self.active_twist_process = subprocess.Popen(cmd)

            except Exception as e:
                print(f"[ERROR] CLI Twist ÌçºÎ∏îÎ¶¨Ïãú Ïã§Ìå®: {e}")

        else:
            print("[ERROR] Î™ÖÎ†π ÌòïÏãùÏù¥ Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏäµÎãàÎã§. (int ÎòêÎäî dict)")

# gstreamer

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mic_index", type=int, default=0, help="Microphone index")
    args = parser.parse_args()

    print("mic_index:", args.mic_index)

    va = VoiceAgent(mic_index=args.mic_index)
    va.loop()

